import re
from copy import deepcopy
from multiprocessing.pool import Pool
from time import sleep
from typing import Any, Dict, List, Optional
from urllib.parse import quote
from datetime import datetime

import requests
import ujson as json
from loguru import logger
from pandas import date_range
import pandas as pd
from tqdm import tqdm
from trafilatura import extract, fetch_url
from trafilatura.settings import DEFAULT_CONFIG

#ÏÑ§Ï†ï
START_DATE = "2025.05.01"
END_DATE = "2025.09.30"
NUM_WORKERS = 6  # Î≥ëÎ†¨ Ï≤òÎ¶¨ Ïàò
MAX_TRIALS = 3   # ÏµúÎåÄ Ïû¨ÏãúÎèÑ ÌöüÏàò
SLEEP_TIME = 1.0 # ÏöîÏ≤≠ Í∞Ñ ÎåÄÍ∏∞ ÏãúÍ∞Ñ

#Í∞ïÏõêÎèÑ 18Í∞ú Ïãú, Íµ∞
REGIONS = [
    #"Ï∂òÏ≤ú", "ÏÜçÏ¥à",
    "Í∞ïÎ¶â",
    #"ÎèôÌï¥", "ÏõêÏ£º", "ÏÇºÏ≤ô",
    #"ÌôçÏ≤ú", "Ìö°ÏÑ±", "ÏòÅÏõî", "ÌèâÏ∞Ω", "Ï†ïÏÑ†",
    #"Ï≤†Ïõê", "ÌôîÏ≤ú", "ÏñëÍµ¨", "Ïù∏Ï†ú", "Í≥†ÏÑ±",
    #"ÏñëÏñë", "ÌÉúÎ∞±"
]

#Î¥á ÏùòÏã¨ Ï∞®Îã® Î∞©ÏßÄ Ìó§Îçî
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36",
    "Referer": "https://m.search.naver.com/",
    "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7"
}

TRAFILATURA_CONFIG = deepcopy(DEFAULT_CONFIG)
TRAFILATURA_CONFIG["DEFAULT"]["DOWNLOAD_TIMEOUT"] = "5"
TRAFILATURA_CONFIG["DEFAULT"]["MIN_OUTPUT_SIZE"] = "50"

#Î≥∏Î¨∏ ÎÇ¥ Î∂àÌïÑÏöî ÎÇ¥Ïö© ÏÇ≠Ï†ú
def clean_news_content(text: str) -> str:
    if not text:
        return ""

    text = re.sub(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}', '', text)

    text = re.sub(r'\[.*?\]', '', text)
    text = re.sub(r'\(.*?\)', '', text)

    text = re.sub(r'\w+\s*Í∏∞Ïûê\s*=', '', text)
    text = re.sub(r'\w+\s*Í∏∞Ïûê', '', text)

    text = re.sub(r'<Ï†ÄÏûëÍ∂åÏûê.*?>', '', text)
    text = re.sub(r'Î¨¥Îã®Ï†ÑÏû¨.*', '', text)
    text = re.sub(r'Ïû¨Î∞∞Ìè¨.*Í∏àÏßÄ', '', text)
    text = re.sub(r'Copyrights.*', '', text, flags=re.IGNORECASE)

    text = re.sub(r'\|', '', text)
    text = re.sub(r'ÎßéÏù¥ Î≥∏ Í∏∞ÏÇ¨', '', text)
    text = re.sub(r'Í¥ÄÎ†® Í∏∞ÏÇ¨', '', text)
    text = re.sub(r'Ïò§ÎäòÏùò Ìï´Îâ¥Ïä§', '', text)
    text = re.sub(r'Í∏∞ÏÇ¨ Ïä§ÌÅ¨Îû©', '', text)

    text = re.sub(r'\n+', ' ', text)
    text = re.sub(r'\s+', ' ', text).strip()

    return text


def get_article_body(url: str) -> Optional[Dict[str, Any]]:
    try:
        downloaded = fetch_url(url, config=TRAFILATURA_CONFIG)
        if downloaded is None:
            return None
            
        extracted = extract(
            downloaded,
            output_format="json",
            target_language="ko",
            with_metadata=True,
            deduplicate=True,
            config=TRAFILATURA_CONFIG,
        )
        
        if not extracted:
            return None
            
        article = json.loads(extracted)
        raw_text = article.get("text", "")
        
        #Ï†ïÏ†úÌï®Ïàò Ìò∏Ï∂ú
        cleaned_text = clean_news_content(raw_text)

        if len(cleaned_text) >= 50:
            article["cleaned_text"] = cleaned_text
            article["source_url"] = url
            return article
            
        return None
    except Exception as e:
        # logger.error(f" Error extracting {url}: {e}")
        return None


def crawl_articles_for_region(region: str) -> List[Dict[str, Any]]:
    query = f"{region} Í∞ÄÎ≠Ñ"
    encoded_query = quote(query)
    dates = date_range(START_DATE, END_DATE, freq="D")
    all_articles = []

    progress_bar = tqdm(total=len(dates), desc=f"üìç {region}", ncols=100)

    for date in dates:
        date_str = date.strftime("%Y%m%d")

        next_url = (
            "https://s.search.naver.com/p/newssearch/3/api/tab/more?"
            f"query={encoded_query}&sort=0&"
            f"nso=so%3Ar%2Cp%3Afrom{date_str}to{date_str}%2Ca%3Aall&ssc=tab.news.all&"
            f"start=1"
        )

        while True:
            num_trials = 0
            response = None
            while num_trials < MAX_TRIALS:
                try:
                    response = requests.get(next_url, headers=HEADERS, timeout=10)
                    response.raise_for_status()
                    break
                except Exception as e:
                    num_trials += 1
                    sleep(SLEEP_TIME)
            
            if response is None:
                break

            try:
                request_result = response.json()
            except Exception:
                break

            if request_result.get("collection") is None:
                break

            next_url = request_result.get("url", "")
            if not next_url:
                break

            script = request_result["collection"][0].get("script", "")
            article_urls = re.findall(r"\"contentHref\":\"(.*?)\"", script)

            if not article_urls:
                break

            # Î≥ëÎ†¨Î°ú Í∏∞ÏÇ¨ Î≥∏Î¨∏ ÌÅ¨Î°§ÎßÅ
            with Pool(NUM_WORKERS) as pool:
                for article in pool.imap_unordered(get_article_body, article_urls):
                    if article:
                        # Í≤∞Í≥º Î¶¨Ïä§Ìä∏ÏóêÎäî ÏßÄÏó≠Î™ÖÍ≥º Ï†ïÏ†úÎêú Î≥∏Î¨∏Îßå Ï†ÄÏû•
                        all_articles.append({
                            "region": region,
                            "text": article["cleaned_text"]
                        })

            sleep(SLEEP_TIME)

        progress_bar.update(1)

    progress_bar.close()
    return all_articles


if __name__ == "__main__":
    logger.info("Í∞ïÏõêÎèÑ 18Í∞ú Ïãú, Íµ∞ Í∞ÄÎ≠Ñ Îâ¥Ïä§ ÌÅ¨Î°§ÎßÅ ÏãúÏûë")

    #ÏöîÏïΩ Ï†ïÎ≥¥ Îã¥ÏùÑ Î¶¨Ïä§Ìä∏
    summary_data = []

    for region in REGIONS:
        logger.info(f"üöó {region} ÏßÄÏó≠ ÏàòÏßë ÏãúÏûë")
        articles = crawl_articles_for_region(region)

        count = 0
        if articles:
            df = pd.DataFrame(articles)
            
            #Ï§ëÎ≥µ Ï†úÍ±∞ (Í∞ôÏùÄ Î≥∏Î¨∏ ÎÇ¥Ïö©ÏùÄ Ï†úÍ±∞)
            df = df.drop_duplicates(subset=['text'])
            
            #Í∞úÎ≥Ñ CSV Ï†ÄÏû• (Ïª¨Îüº: region, text)
            df = df[["region", "text"]]
            filename = f"Í∞ÄÎ≠Ñ_{region}.csv"
            df.to_csv(filename, index=False, encoding="utf-8-sig")
            
            count = len(df)
            logger.success(f"{region}: {count}Í∞ú Í∏∞ÏÇ¨ Ï†ÄÏû• ÏôÑÎ£å ‚Üí {filename}")
        else:
            logger.warning(f"{region}: ÏàòÏßëÎêú Îâ¥Ïä§Í∞Ä ÏóÜÏäµÎãàÎã§.")

        #ÏöîÏïΩ Îç∞Ïù¥ÌÑ∞ Ï∂îÍ∞Ä
        summary_data.append({"region": region, "count": count})

    #Ï†ÑÏ≤¥ ÏöîÏïΩ ÌååÏùº Ï†ÄÏû• (Ïª¨Îüº: region, count)
    summary_df = pd.DataFrame(summary_data)
    summary_filename = "Í∞ïÏõêÎèÑ_ÏßÄÏó≠Î≥Ñ_Îâ¥Ïä§Í∞ØÏàò.csv"
    summary_df.to_csv(summary_filename, index=False, encoding="utf-8-sig")
    
    logger.success(f"Ï†ÑÏ≤¥ ÏöîÏïΩ ÌååÏùº Ï†ÄÏû• ÏôÑÎ£å ‚Üí {summary_filename}")
    logger.success("Ï†ÑÏ≤¥ ÏßÄÏó≠ ÌÅ¨Î°§ÎßÅ Ï¢ÖÎ£å")