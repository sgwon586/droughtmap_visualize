import re
from copy import deepcopy
from multiprocessing.pool import Pool
from time import sleep
from typing import Any, Dict, List, Optional
from urllib.parse import quote
from datetime import datetime

import requests
import ujson as json
from loguru import logger
from pandas import date_range
import pandas as pd
from tqdm import tqdm
from trafilatura import extract, fetch_url
from trafilatura.settings import DEFAULT_CONFIG


#ÏÑ§Ï†ï
START_DATE = "2025.05.01"
END_DATE = "2025.10.31"
NUM_WORKERS = 6 # Î≥ëÎ†¨ Ï≤òÎ¶¨ Ïàò
MAX_TRIALS = 3 # ÏµúÎåÄ Ïû¨ÏãúÎèÑ ÌöüÏàò
SLEEP_TIME = 1.0 # ÏöîÏ≤≠ Í∞Ñ ÎåÄÍ∏∞ ÏãúÍ∞Ñ

# Í∞ïÏõêÎèÑ 18Í∞ú Ïãú, Íµ∞
REGIONS = [
    "Ï∂òÏ≤ú", "ÏÜçÏ¥à", "Í∞ïÎ¶â",
    #"ÎèôÌï¥", "ÏõêÏ£º", "ÏÇºÏ≤ô",
    #"ÌôçÏ≤ú", "Ìö°ÏÑ±", "ÏòÅÏõî", "ÌèâÏ∞Ω", "Ï†ïÏÑ†",
    #"Ï≤†Ïõê", "ÌôîÏ≤ú", "ÏñëÍµ¨", "Ïù∏Ï†ú", "Í≥†ÏÑ±",
    #"ÏñëÏñë", "ÌÉúÎ∞±"
]


TRAFILATURA_CONFIG = deepcopy(DEFAULT_CONFIG)
TRAFILATURA_CONFIG["DEFAULT"]["DOWNLOAD_TIMEOUT"] = "5" #ÏµúÎåÄ 5Ï¥àÍπåÏßÄÎßå ÎåÄÍ∏∞
TRAFILATURA_CONFIG["DEFAULT"]["MIN_OUTPUT_SIZE"] = "50" #Î≥∏Î¨∏Ïù¥ 50Ïûê Ïù¥ÏÉÅÏù∏ Í≤ΩÏö∞Îßå


# Îâ¥Ïä§ Î≥∏Î¨∏ Ï∂îÏ∂ú Ìï®Ïàò
def get_article_body(url: str) -> Optional[Dict[str, Any]]:
    """Ï£ºÏñ¥ÏßÑ Í∏∞ÏÇ¨ URLÏóêÏÑú Î≥∏Î¨∏ Ï∂îÏ∂ú"""
    try:
        downloaded = fetch_url(url, config=TRAFILATURA_CONFIG)
        extracted = extract(
            downloaded,
            output_format="json",
            target_language="ko",
            with_metadata=True,
            deduplicate=True,
            config=TRAFILATURA_CONFIG,
        )
        if not extracted:
            return None
        article = json.loads(extracted)
        if "text" in article and len(article["text"]) >= 50:
            article["source_url"] = url
            return article
        return None
    except Exception as e:
        logger.error(f" Error extracting {url}: {e}")
        return None


# ÎÇ†ÏßúÎ≥Ñ Îâ¥Ïä§ ÏàòÏßë Ìï®Ïàò
def crawl_articles_for_region(region: str) -> List[Dict[str, Any]]:
    query = f"{region} Í∞ÄÎ≠Ñ"
    encoded_query = quote(query)
    dates = date_range(START_DATE, END_DATE, freq="D")
    all_articles = []

    progress_bar = tqdm(total=len(dates), desc=f"üìç {region}", ncols=100)

    for date in dates:
        date_str = date.strftime("%Y%m%d")

        next_url = (
            "https://s.search.naver.com/p/newssearch/3/api/tab/more?"
            f"query={encoded_query}&sort=0&"
            f"nso=so%3Ar%2Cp%3Afrom{date_str}to{date_str}%2Ca%3Aall&ssc=tab.news.all&"
            f"start=1"
        )

        while True:
            num_trials = 0
            while num_trials < MAX_TRIALS:
                try:
                    response = requests.get(next_url, timeout=10)
                    break
                except Exception as e:
                    num_trials += 1
                    logger.warning(f"Retrying {next_url} ({num_trials}/{MAX_TRIALS}) due to {e}")
                    sleep(SLEEP_TIME)
            else:
                logger.error(f"Failed to fetch data for {date_str} after {MAX_TRIALS} retries")
                break

            try:
                request_result = response.json()
            except Exception:
                logger.warning(f"Invalid JSON on {date_str}")
                break

            if request_result.get("collection") is None:
                break

            next_url = request_result.get("url", "")
            if not next_url:
                break

            script = request_result["collection"][0].get("script", "")
            article_urls = re.findall(r"\"contentHref\":\"(.*?)\"", script)

            if not article_urls:
                break

            # Î≥ëÎ†¨Î°ú Í∏∞ÏÇ¨ Î≥∏Î¨∏ ÌÅ¨Î°§ÎßÅ
            with Pool(NUM_WORKERS) as pool:
                for article in pool.imap_unordered(get_article_body, article_urls):
                    if article:
                        article["region"] = region
                        article["date_crawled"] = date_str
                        all_articles.append(article)

            sleep(SLEEP_TIME)

        progress_bar.update(1)

    progress_bar.close()
    return all_articles


# Î©îÏù∏ Ìï®Ïàò
if __name__ == "__main__":
    logger.info("Í∞ïÏõêÎèÑ 18Í∞ú Ïãú, Íµ∞ Í∞ÄÎ≠Ñ Îâ¥Ïä§ ÌÅ¨Î°§ÎßÅ ÏãúÏûë")

    for region in REGIONS:
        logger.info(f"üöó {region} ÏßÄÏó≠ ÏàòÏßë ÏãúÏûë")
        articles = crawl_articles_for_region(region)

        if not articles:
            logger.warning(f"{region}: ÏàòÏßëÎêú Îâ¥Ïä§Í∞Ä ÏóÜÏäµÎãàÎã§.")
            continue

        df = pd.DataFrame(articles)
        df = df[["region", "title", "author", "date", "text", "source_url"]]

        filename = f"Í∞ÄÎ≠Ñ_{region}.csv"
        df.to_csv(filename, index=False, encoding="utf-8-sig")
        logger.success(f"üòä {region}: {len(df)}Í∞ú Í∏∞ÏÇ¨ Ï†ÄÏû• ÏôÑÎ£å ‚Üí {filename}")

    logger.success("Ï†ÑÏ≤¥ ÏßÄÏó≠ ÌÅ¨Î°§ÎßÅ ÏôÑÎ£å")
